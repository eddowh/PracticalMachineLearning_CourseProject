---
title: "Practical Machine Learning Course Project"
author: "Eddo W. Hintoso"
date: "September 22, 2015"
output:
    html_document:
        keep_md: true
        toc: yes
        theme: readable
        highlight: tango
---

```{r, warning = FALSE, message = FALSE, echo = FALSE}
library(knitr)
opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
```


# Processing Data

First and foremost, download the files and load necessary packages:
```{r, warning = FALSE, message = FALSE}
# load package if not yet loaded
if(!("caret" %in% loadedNamespaces())) {
    library(caret)
}
# download data
if(!file.exists("pml-training.csv")){
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
        destfile = "pml-training.csv")
}
if(!file.exists("pml-testing.csv")){
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
        destfile = "pml-testing.csv")
}
# load data
train <- read.csv('pml-training.csv', header = TRUE, na.strings = c("", "NA", "#DIV/0!"))
test <- read.csv('pml-testing.csv', header = TRUE, na.strings = c("", "NA", "#DIV/0!"))
```

Note that I have treated some string values as `NA`. This is because upon primary inspection of the data there were many missing data that best be uniformly expressed as `NA`. The below R code is shown on how the author came to detect missing values - however it is only for display and will not be evaluated due to the length of the output exceeding the degree of readability. 
```{r, eval = FALSE}
# check if there any NA in any column variables of train
TRUE %in% sapply(train, function(col) {NA %in% col})
# individually inspect odd outputs by iterating over factor and character variables
# helps to see the factor levels
for (col in 1:ncol(train)) {
    if(class(train[, col]) == "factor" |
       class(train[, col]) == "character") {
        print(unique(train[, col]))
    }
}
```

In order to run the machine learning algorithms, the features used cannot contain any `NA` values. Let us quickly inspect how many variables are complete:
```{r}
completionStatus <- sapply(train, function(col) {!(NA %in% col)})
c("Complete Variables" = sum(completionStatus),
  "Total Variables" = length(completionStatus),
  "Percent Completion" = 100 * sum(completionStatus) / length(completionStatus))
```

We now know that only 60 variables have complete data - so we will only use these column variables for our predictor models, since imputing data carries a considerable risk and may affect the accuracy of the predictor model we're going to fit. Think of this as taking a measure to potentially overfit a predictor model due to too many column variables. Now all that is left is filtering the training and testing data sets from the incomplete data columns:
```{r}
# initialize index to empty vector
complete_index = c()
# iterate from second column since first column is just row index
for (col in 2:length(completionStatus)) {
    if (completionStatus[[col]] == TRUE) {
        complete_index = c(complete_index, col)
    }
}
# filter training and testing dataset into just complete data columns
train <- train[, complete_index]
test <- test[, complete_index]
```

Both the training and testing data should now only have 59 column variables, and this is confirmed below:
```{r}
c(ncol(train), ncol(test))
```

However, this isn't the last step to preparing a useful training data set. Upon further inspection, some column variables can be argued to not being contributable to a predicting model.
```{r}
# infer from variable names
names(train)
# examine structure
str(train[, 1:10])
```

It can be inferred that the first six variables ``r names(train)[1:6]`` are administrative, integer/factor variables, unlike the other numeric variables that serve to contribute to building a good predictive model. Thus, more variable elimination is required, bringing it down to 53 variables.
```{r}
# eliminate first 6 columns
train <- train[, -(1:6)]
test <- test[, -(1:6)]
# check dimensions
c(ncol(train), ncol(test))
```

---


# Cross Validation

We set `test` set aside and split the `train` data into two sections for cross validation. We will allocate 70% of the data to train the model and 30% to validate it.

We expect that the **out-of-bag (OOB)** error rates returned by the models should be good estimate for the out of sample error rate. We will get actual estimates of error rates from the **accuracies** achieved by the models.
```{r}
# set seed
set.seed(3433)
# split train data set
inTrain <- createDataPartition(train$classe, p = 0.7, list = FALSE)
trainData <- train[inTrain, ]
validation <- train[-inTrain, ]
# print out dimensions of each data sets
rbind(trainData = dim(trainData), validation = dim(validation), test = dim(test))
```

---

# Comparing Models

For this project, I choose to predict the `classe` variable with all the other variables using a **random forest (`“rf”`)** and **boosted trees (`“gbm”`)**. Finally, I will stack the predictions together using random forests (“rf”) for a combined model.

First, however, we will use parallel processing capabilities to speed up the training speed, since creating four predictor models is computationally expensive.
```{r}
# process in parallel
library(doParallel)
registerDoSEQ()
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
```

Now we are ready to fit the model predictors, but not without setting a seed first:
```{r}
# set seed
set.seed(62433)
# load packages
library(randomForest)
# fitting random forest model predictor and record elapsed time, printing out results
elapsedFitRF <- system.time(
    print(
        fitRF <- randomForest(classe ~ ., data=trainData, method="rf")
    )
)
```

```{r}
# fitting boosted trees model predictor and record elapsed time, printing out results
library(gbm)
elapsedFitGBM <- system.time(
    print(
        fitGBM <- train(classe ~ ., data=trainData, method="gbm", verbose=FALSE)
    )
)
```

After we have trained our models, we predict:
```{r}
# predict using model predictors and record elapsed time
elapsedPredRF <- system.time(
    predRF <- predict(fitRF, newdata=validation)
)
elapsedPredGBM <- system.time(
    predGBM <- predict(fitGBM, newdata=validation)
)

# create new dataframe for stacking predictors
predAll <- data.frame(predRF, predGBM, classe = validation$classe)
elapsedFitStacked <- system.time(
    fitStacked <- randomForest(classe ~ ., data=predAll, method = 'rf')
)

# predicting with stacked predictors
elapsedPredStacked <- system.time(
    predStacked <- predict(fitStacked, newdata=validation)
)
```

From the above, we can see that **randomForest** is the better performing algorithm with **0.46% out-of-bag (OOB) error rate**, which is what we expect the out of sample error rate to be.

---

# Run-Time Graphical Analysis of Models

In this section we will attempt to see what is the best model to use, considering trade-offs. First, we need to the confusion matrices containing analysis of the models into variables:
```{r}
# confusion matrices
cmRF <- confusionMatrix(predRF, validation$classe)
cmGBM <- confusionMatrix(predGBM, validation$classe)
cmStacked <- confusionMatrix(predStacked, validation$classe)

# create table
analysis_table <- data.frame("Model" = c("Random Forest",
                                          "Generalized Boosted Trees",
                                          "Random Forest + GBM Stacked"),
                             "Accuracy" = 100 * c(cmRF$overall[[1]],
                                                  cmGBM$overall[[1]],
                                                  cmStacked$overall[[1]]),
                             "Training Speed" = c(elapsedFitRF[['elapsed']],
                                                  elapsedFitGBM[['elapsed']],
                                                  (elapsedFitRF[['elapsed']] +
                                                       elapsedFitGBM[['elapsed']] +
                                                       elapsedFitStacked[['elapsed']])),
                             "Prediction Speed" = c(elapsedPredRF[['elapsed']],
                                                    elapsedPredGBM[['elapsed']],
                                                    (elapsedPredRF[['elapsed']] +
                                                         elapsedPredGBM[['elapsed']] +
                                                         elapsedPredStacked[['elapsed']])))
                             
names(analysis_table) <- c('Model', 'Accuracy', 'Training Speed (sec)', 'Prediction Speed (sec)')

# round numeric columns
analysis_table[, 2:4] <- round(analysis_table[, 2:4], digits = 2)

# display table nicely
kable(analysis_table,
      align = "c")
```

To better visualize the run-time results, we can also make a bar graph:
```{r}
library(ggplot2)
# accuracy comparisons
accuracy_plot <- ggplot(transform(analysis_table,
                                  Model = reorder(Model, Accuracy)),
                        aes(x = Model, y = Accuracy)) +
    geom_bar(stat="identity",
             aes(fill = Accuracy == max(Accuracy)),
             position=position_dodge()) +
    scale_fill_discrete(guide = 'none') + 
    labs(x = 'Model',
         y = 'Rate (%)',
         title = 'Model Accuracy') +
    coord_flip()
# training speed comparisons
train_speed_plot <- ggplot(transform(analysis_table,
                                     Model = reorder(Model, analysis_table[, 3])),
                           aes(x = Model, y = analysis_table[, 3])) + 
    geom_bar(stat="identity",
             aes(fill = analysis_table[, 3] == min(analysis_table[, 3])),
             position=position_dodge()) + 
    labs(x = 'Model',
         y = 'Time (sec)',
         title = 'Training Speed') + 
    scale_fill_discrete(guide = 'none') + 
    coord_flip()
# prediction speed comparisons
pred_speed_plot <- ggplot(transform(analysis_table,
                                    Model = reorder(Model, analysis_table[, 4])),
                          aes(x = Model, y = analysis_table[, 4])) + 
    geom_bar(stat="identity",
             aes(fill = analysis_table[, 4] == min(analysis_table[, 4])), 
             position=position_dodge()) + 
    labs(x = 'Model',
         y = 'Time (sec)',
         title = 'Prediction Speed') + 
    scale_fill_discrete(guide = 'none') + 
    coord_flip()
# plot all three at once
library(gridExtra)
grid.arrange(accuracy_plot,
             train_speed_plot,
             pred_speed_plot,
             ncol = 1)
```

As one can see from above, the *best accuracy rate* belongs to **Random Forest and GBM stacked together** and **Random Forest** - both accuracy values are identical.

The *shortest training speed* belongs to **Random Forest**, by a huge margin.

The *shortest prediction speed* belongs to **Generalized Boosted Trees**, but only by a matter of seconds, so the difference is trivial.

---

# Result

Given the analysis presented, there is no doubt that the best accuracy combined with best time efficiency belongs to **Random Forest**. Thus this will be our model of choice in predicting the `test` set.

```{r}
print(
    test_result <- predict(fitRF, test)
)
```

```{r}
# save results into separate files in appropriate directory
source('./pml_writing_files.R')
pml_write_files(as.character(data.frame(test_result)$test_result))
```

